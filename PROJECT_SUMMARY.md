# Transfer Learning for Building Energy Forecasting

## Project Overview

This project implements a **3-model transfer learning framework** for building energy consumption forecasting using LSTM neural networks. The goal is to demonstrate that transfer learning can improve prediction accuracy when only limited data is available for a new building.

## ðŸŽ¯ Experimental Design

### Models

1. **Baseline Model** (`src/train_baseline.py`)
   - **Purpose**: Learn general patterns from abundant source data
   - **Training**: 2 years of data from Rat_education_Colin (~17,500 hours)
   - **Architecture**: 3-layer LSTM (128 hidden units, seq_length=168)
   - **Data Split**: Stratified random split (avoids seasonal distribution mismatch)

2. **Pre-Transfer Model** (`src/train_pretransfer.py`)
   - **Purpose**: Control group - train from scratch on limited target data
   - **Training**: 8 weeks of Rat_education_Denise data (~1,344 hours)
   - **Architecture**: Simplified (2 layers, 64 hidden units, seq_length=24)
   - **Data Split**: Stratified random split

3. **Transfer Model** (`src/train_transfer.py`)
   - **Purpose**: Experimental group - fine-tune baseline on limited target data  
   - **Training**: Same 8 weeks of Rat_education_Denise
   - **Initialization**: Loaded from baseline model weights
   - **Architecture**: Matches baseline (adapted for shorter sequences)

4. **Data Efficiency Models** (`train_data_efficiency.py`)
   - **Purpose**: Evaluate how performance scales with data amount
   - **Training**: Both Pre-Transfer and Transfer with 1, 2, 4, 8, 16 weeks
   - **Building**: Rat_education_Denise
   - **Output**: Performance comparison tables for all data amounts

### Key Design Decisions

âœ… **Stratified Random Split**: All models use stratified random splits by month to ensure train/val/test have similar distributions. This avoids the 52% distribution shift that caused negative RÂ² in early experiments.

âœ… **Same Target Data**: Pre-Transfer and Transfer use identical data (8 weeks) to isolate the effect of transfer learning.

âœ… **Adaptive Architecture**: Limited-data models use simpler architecture (64 hidden, 2 layers) to prevent overfitting with small datasets.

## ðŸ“Š Research Questions

1. **Does transfer learning improve performance with limited data?**
   - Compare: Pre-Transfer RMSE vs Transfer RMSE on target building

2. **How much improvement does transfer learning provide?**
   - Measure: % reduction in RMSE, MAE, and improvement in RÂ²

3. **Can we overcome distribution shifts across buildings?**
   - Compare: Baseline on source vs Baseline on target

## ðŸš€ Usage

### Train All Models

```bash
# Option 1: Train individually
python src/train_baseline.py       # ~15-25 epochs
python src/train_pretransfer.py    # ~100 epochs  
python src/train_transfer.py       # ~50 epochs

# Option 2: Automated pipeline
python run_training_pipeline.py
```

### Evaluate & Compare

```bash
python evaluate_all_models.py
```

This generates:
- `results/three_model_comparison.csv` - Detailed metrics
- `results/model_comparison.png` - Visualization
- Console output with comprehensive analysis

## ðŸ“ˆ Expected Results

```
Model Comparison:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model          Data Source        RMSE (kWh)  RÂ²     Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Baseline       2yr Colin         <15         >0.6   Best case
Pre-Transfer   2mo Denise        ~20-30      ~0.4   Control
Transfer       2mo Denise        <20         >0.6   â†‘ Improved!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Success Criteria**: Transfer RMSE < Pre-Transfer RMSE (proves transfer learning helps!)

## ðŸ”§ Technical Details

### Data Processing
- **Dataset**: Building Data Genome Project 2 (Education buildings, Rat site)
- **Features**: Weather data + temporal features (31 total)
- **Normalization**: StandardScaler on features, energy target unscaled
- **Train/Val/Test**: 60/20/20 split with month-based stratification

### Model Architecture
```python
# Baseline (abundant data)
LSTM: 3 layers Ã— 128 hidden units
Sequence: 168 hours (1 week)
Dropout: 0.2
Learning rate: 5e-4

# Pre-Transfer & Transfer (limited data)
LSTM: 2 layers Ã— 64 hidden units  
Sequence: 24 hours (1 day)
Dropout: 0.2
Learning rate: 1e-3 (pre-transfer), 1e-4 (transfer)
```

### Critical Fixes Applied

âš ï¸ **Distribution Mismatch Issue**: Initial chronological split caused 52% mean shift between train (60.8 kWh) and test (29.2 kWh), resulting in negative RÂ². Fixed by implementing stratified random split.

âœ… **Early Stopping**: Increased patience from 10 to 15 epochs to allow proper convergence.

âœ… **Sequence Length**: Reduced from 336 to 168 hours for baseline (easier for LSTM to learn).

## ðŸ“ Project Structure

```
energy-transfer-learning/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py          # Data loading & preprocessing
â”‚   â”œâ”€â”€ models.py               # LSTM model definition
â”‚   â”œâ”€â”€ train_baseline.py       # Train baseline model
â”‚   â”œâ”€â”€ train_pretransfer.py    # Train from scratch (control)
â”‚   â””â”€â”€ train_transfer.py       # Fine-tune baseline (experimental)
â”œâ”€â”€ evaluate_all_models.py      # Compare all 3 models
â”œâ”€â”€ run_training_pipeline.py    # Automated training
â”œâ”€â”€ models/                      # Saved model checkpoints
â”œâ”€â”€ results/                     # Evaluation results
â””â”€â”€ data/                        # Building Data Genome Project 2
```

## ðŸ“š Key Findings from Development

1. **Data Distribution**: Education buildings show strong seasonal patterns. Test set (Aug-Dec) had 52% lower consumption than train set (Jan-Jul), likely due to school closures. Stratified split resolved this.

2. **Model Convergence**: Initial training stopped too early (epoch 7) with val_RMSE=34 kWh. With improved settings, models converge to <15 kWh RMSE.

3. **Architecture Scaling**: Full baseline architecture (128 hidden, 3 layers, 336 seq_length) doesn't work with limited data. Reduced to (64 hidden, 2 layers, 24 seq_length) for limited-data scenarios.

## ðŸŽ“ Research Context

This project follows standard transfer learning experimental design:
- **Source task**: Energy forecasting for building with abundant data
- **Target task**: Energy forecasting for building with limited data
- **Hypothesis**: Pre-trained model improves performance vs training from scratch
- **Comparison**: Controlled (Pre-Transfer) vs Experimental (Transfer)

## ðŸ”„ Next Steps

1. âœ… Train all models with fixed data splits
2. â³ Evaluate and document final results
3. â³ Statistical significance testing (t-test on improvements)
4. â³ Analyze which features transfer best
5. â³ Try different target buildings to test generalization

---

**Status**: Core implementation complete. Ready for training and evaluation.
